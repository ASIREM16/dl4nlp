# Module 2

In the second module, we will focus on NLP tasks where the goal is to predict a structured object such a sequence or a tree. Applications include well-known use cases such as named entity recognition and syntactic parsing.

## Lecture 1

**Form:** Zoom meeting, Monday, 4 May, 10:15–12

* Introduction to the module [[slides](slides/module2.pdf)] [[video](https://youtu.be/PK0Kil5REy8)]

**Reading:** Eisenstein, chapter 1

## Lecture 2

**Form:** Self-study

* Introduction to sequence prediction tasks [[slides](slides/slides-221.pdf)] [[video](https://youtu.be/VCORDrz-Tzs)]
* Basic models for sequence labeling [[slides](slides/slides-222.pdf)] [[video](https://youtu.be/E7jrhDkrmZQ)]
* Subword representations [[slides](slides/slides-223.pdf)] [[video](https://youtu.be/1ZDpYspEM_M)]

**Reading:** Eisenstein, 7.1, 7.6, 8

## Exercise 1

**Form:** Zoom meeting, Monday, 11 May, 13:15–15:00

* Named entity recognition 1 [[code](https://github.com/liu-nlp/dl4nlp/tree/master/exercise2_1)] [[colab](https://drive.google.com/file/d/1xLwc_NGpqscRfJaQAITmE5CoTRqksJAz/view)] [[solution](https://github.com/liu-nlp/dl4nlp/blob/master/exercise2_1/Exercise%202.1%20solution.ipynb)]

## Lecture 3

**Form:** Self-study

* Autoregressive sequence models [[slides](slides/slides-231.pdf)] [[video](https://youtu.be/V9TJMODq-rU)]
* Factorized sequence model and the Viterbi algorithm [[slides](slides/slides-232.pdf)] [[video](https://youtu.be/C_5nfLIhMjw)]
* Conditional random fields [[slides](slides/slides-233.pdf)] [[video](https://youtu.be/8wLScZOGeRc)]

**Reading:**

## Exercise 2

**Form:** Zoom, date and time TBA

* Named entity recognition 2

## Lecture 4

**Form:** Self-study

* Introduction to dependency parsing [[slides](slides/slides-241.pdf)] [[video](https://youtu.be/cx4B43sstTQ)]
* The arc-standard algorithm [[slides](slides/slides-242.pdf)] [[video](https://youtu.be/IQC8Qy8bfG8)]
* Dynamic oracles and imitation learning

**Reading:** 

## Lecture 5

**Form:** Self-study

* The Eisner algorithm [[slides](slides/slides-251.pdf)] [[video](https://youtu.be/QU059k3xifI)]
* Neural parsing architectures [[slides](slides/slides-252.pdf)] [[video](https://youtu.be/l3_HM9NfSjc)]

**Reading:** 

* [Chen and Manning (2014)](https://www.aclweb.org/anthology/D14-1082/)
* [Kiperwasser and Goldberg (2016)](https://www.aclweb.org/anthology/Q16-1023/)
* [Dozat and Manning (2017)](https://openreview.net/forum?id=Hk95PK9le)

## Discussion

**Form:** Zoom, Tuesday, 25 May, 13:00–16:00 (after the project pitches)

* Introduction to Assignment 2

## Discussion

**Form:** Zoom, Tuesday, 25 May, 13:00–16:00

* Project pitch

* For a description of the organization of the project, see [this page](project.md).

## Programming assignment

[The second programming assignment](assignment2/assignment2.ipynb) is dedicated to the task of *dependency parsing*.

You can find a recording of the introduction to the assignment [here](https://youtu.be/C7PnsTie1YA).