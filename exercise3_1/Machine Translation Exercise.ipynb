{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP: Machine Translation Exercise\n",
    "\n",
    "In this exercise, we'll carry out some experiments where we investigate the famous attention-based encoder–decoder model presented by [Bahdanau et al. (2015)](https://arxiv.org/pdf/1409.0473.pdf). In particular, we'll take a look at the attention model.\n",
    "\n",
    "Acknowledgement: the decoder and attention parts have been adapted from [The Annotated Encoder Decoder](https://bastings.github.io/annotated_encoder_decoder/) by Joost Bastings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll have to install the [SacreBLEU](https://github.com/mjpost/sacrebleu) library, which we'll use to compute BLEU scores. It should be enough to do `pip3 install sacrebleu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous exercise, we move some utility code into a separate file `mt_util.py` to make this notebook a bit shorter. The utility functions are almost identical to those we used in the sequence labeling exercise:\n",
    "* `read_lines` to read the tokenized sentences. You'll have to call this for each language separately.\n",
    "* `Vocabulary` to manage the vocabulary and encode words as integers.\n",
    "* `SequenceDataset` and `SequenceBatcher` to manage the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mt_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at the data\n",
    "\n",
    "Two parallel datasets have been prepared for this exercise:\n",
    "* a [German–English dataset](http://www.cse.chalmers.se/~richajo/waspnlp2020/ex3_1/de_en.zip) taken from translations of TED talks.\n",
    "* a [Swedish–English dataset](http://www.cse.chalmers.se/~richajo/waspnlp2020/ex3_1/sv_en.zip) (smaller package [here](http://www.cse.chalmers.se/~richajo/waspnlp2020/ex3_1/sv_en_small.zip)) from the European Parliament proceedings.\n",
    "\n",
    "These datasets have been tokenized and aligned at the sentence level. To make our life a bit easier, we've also removed sentences that are longer than 25 words. You can use other datasets if you want (see e.g. [OPUS](http://opus.nlpl.eu/)) but please don't spend too much time preparing the data.\n",
    "\n",
    "Download the data and take a look at the first sentences. Here's how we can do this for the German–English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_sentences = mt_util.read_lines('de_en/de_en_valid.de', max_lines=10)\n",
    "english_sentences = mt_util.read_lines('de_en/de_en_valid.en', max_lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for de, en in zip(german_sentences, english_sentences):\n",
    "    print(' '.join(de), '/', ' '.join(en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "Now we'll define the encoder–decoder neural network.\n",
    "\n",
    "The high-level container `EncoderDecoder` just wraps the encoder and decoder. We define two methods that we'll use for training and for running the translator, respectively.\n",
    "\n",
    "* `forward` computes output scores for *teacher forcing* during training.\n",
    "* `greedy_decode` applies the decoder in a step-by-step manner, selecting the top-scoring word at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    # S is a batch of source-language sentences, shape (n_sentences, n_source_words).\n",
    "    # T_shifted contains the corresponding target-language sentences, shifted one step\n",
    "    #   to the right for teacher forcing, shape (n_sentences, n_target_words).\n",
    "    def forward(self, S, T_shifted):        \n",
    "        # Apply the encoder, we get token representations in the source language.\n",
    "        S_enc = self.encoder(S)\n",
    "\n",
    "        # Compute a mask that we'll use to get rid of padding tokens when we compute\n",
    "        # the attention. (We hardcode that the padding token has index 0.)\n",
    "        S_mask = (S != 0)\n",
    "        \n",
    "        # Apply the decoder in teacher forcing mode.\n",
    "        return self.decoder(S_enc, S_mask, T_shifted)\n",
    "    \n",
    "    # S is a batch of source-language sentences, shape (n_sentences, n_source_words).\n",
    "    # max_len is the maximally allowed sentence length.\n",
    "    def greedy_decode(self, S, max_len):\n",
    "        S_enc = self.encoder(S)    \n",
    "        S_mask = (S != 0)\n",
    "        return self.decoder.greedy_decode(S_enc, S_mask, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder\n",
    "\n",
    "The encoder is very straightforward. We look up word embeddings and then apply an RNN (a bidirectional GRU in this case) to compute a representation at each token position.\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/waspnlp2020/ex3_1/encoder.svg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_layer, param):\n",
    "        super().__init__()\n",
    "        _, emb_dim = emb_layer.weight.shape       \n",
    "        self.emb_layer = emb_layer\n",
    "        self.rnn = nn.GRU(emb_dim, param.enc_rnn_dim, param.enc_rnn_depth, \n",
    "                          batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, S):\n",
    "        embedded = self.emb_layer(S)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder is also based on a GRU, but this will have to be unidirectional since we're generating in one direction. The GRU is called $f$ in [Bahdanau's paper](https://arxiv.org/pdf/1409.0473.pdf), Section 3.1.\n",
    "The input to this GRU is a combination of a word embedding of the previous word and a *context* computed by an attention model. This context contains \"useful\" information from the encoded source sentence.\n",
    "\n",
    "The output from the GRU is fed into an output word prediction unit, and combined with the previous word embedding and the context. In this implementation, this output unit is a linear model. This part is called $g$ in the paper.\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/waspnlp2020/ex3_1/decoder.svg\" alt=\"Drawing\" style=\"width: 180px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_layer, param):\n",
    "        super().__init__()\n",
    "        voc_size, emb_dim = emb_layer.weight.shape\n",
    "\n",
    "        # Word embeddings for the target language. \n",
    "        self.emb_layer = emb_layer\n",
    "\n",
    "        attn_dim = 2*param.enc_rnn_dim\n",
    "\n",
    "        # The decoder's state is represented using a GRU, called f in Bahdanau's paper.\n",
    "        # The input of this GRU is a combination of the embedding of the previous word\n",
    "        # and the \"context\" computed by the attention model.\n",
    "        self.rnn = nn.GRU(emb_dim + attn_dim, param.dec_rnn_dim, param.dec_rnn_depth, \n",
    "                          batch_first=True, bidirectional=False)\n",
    "        \n",
    "        # The output layer (called g in the paper) is a bit simpler than in the paper. \n",
    "        # We just use a linear model.\n",
    "        self.output_layer = nn.Linear(param.dec_rnn_dim + attn_dim + emb_dim,\n",
    "                                      voc_size, bias=False)\n",
    "                \n",
    "        self.rnn_dim = param.dec_rnn_dim\n",
    "\n",
    "        # The attention model, which computes a context or \"summary\" of the source sentence.\n",
    "        # Right now, we use a simplified attention model that doesn't depend on the decoder state.\n",
    "        # See below for the implementation.\n",
    "        self.attention = MeanAttention()\n",
    "        \n",
    "        \n",
    "    # This method applies the attention model, the RNN and the output model at one\n",
    "    # position in the decoded sentence.\n",
    "    def forward_step(self, prev_embed, enc_out, src_mask, precomputed_key, decoder_hidden):        \n",
    "\n",
    "        # The \"query\" for attention is the hidden state of the decoder.\n",
    "        query = decoder_hidden[-1].unsqueeze(1)  \n",
    "        \n",
    "        # Apply the attention model to compute a \"context\", summary of the encoder output\n",
    "        # based on the current query.\n",
    "        # Also returns the attention weights, which we might want to visualize or inspect.\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, precomputed_key=precomputed_key,\n",
    "            value=enc_out, mask=src_mask)\n",
    "        \n",
    "        # Run the RNN one step.\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        rnn_output, decoder_hidden = self.rnn(rnn_input, decoder_hidden)\n",
    "        \n",
    "        # The word output model (called g in Bahdanau's paper) uses the embedding \n",
    "        # of the previous word, the RNN output, and the context computed by the attention model.\n",
    "        pre_output = torch.cat([prev_embed, rnn_output, context], dim=2)\n",
    "        T_output = self.output_layer(pre_output)\n",
    "\n",
    "        return attn_probs, decoder_hidden, T_output\n",
    "        \n",
    "        \n",
    "    # Apply the decoder in teacher forcing mode.\n",
    "    def forward(self, enc_out, src_mask, T_shifted):\n",
    "\n",
    "        n_sen, n_words = T_shifted.shape\n",
    "        \n",
    "        # Word embeddings for the shifted word, representing the previous step.\n",
    "        T_emb = self.emb_layer(T_shifted)\n",
    "        \n",
    "        # Precompute attention keys (if needed).\n",
    "        precomputed_key = self.attention.precompute_key(enc_out)\n",
    "        \n",
    "        # Initialize the hidden state of the GRU.\n",
    "        decoder_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
    "        \n",
    "        all_out = []\n",
    "        \n",
    "        # For each position in the target sentence:\n",
    "        for i in range(n_words):\n",
    "            \n",
    "            # Embedding for the previous word.\n",
    "            prev_embed = T_emb[:, i].unsqueeze(1)\n",
    "            \n",
    "            # Run the decoder one step.\n",
    "            # This returns a new hidden state, and the output \n",
    "            # scores (over the target vocabulary) at this position.\n",
    "            _, decoder_hidden, T_output = self.forward_step(prev_embed, enc_out, \n",
    "                                                            src_mask, precomputed_key, decoder_hidden)\n",
    "            all_out.append(T_output)\n",
    " \n",
    "        # Combine the output scores for all positions.\n",
    "        return torch.cat(all_out, dim=1)\n",
    "            \n",
    "    # Apply the decoder in a greedy step-by-step fashion, at each step selecting the\n",
    "    # highest-scoring word.\n",
    "    def greedy_decode(self, enc_out, src_mask, max_len):\n",
    "        n_sen, _ = src_mask.shape\n",
    "\n",
    "        precomputed_key = self.attention.precompute_key(enc_out)\n",
    "        decoder_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
    "        \n",
    "        prev_tokens = torch.zeros(src_mask.size(0), 1, dtype=torch.long, \n",
    "                                  device=src_mask.device)\n",
    "        all_out = []\n",
    "        all_attn = []\n",
    "\n",
    "        for i in range(max_len):\n",
    "            prev_embed = self.emb_layer(prev_tokens)\n",
    "            attn_probs, decoder_hidden, T_output = self.forward_step(prev_embed, enc_out, src_mask, \n",
    "                                                                     precomputed_key, decoder_hidden)\n",
    "            # Select the highest-scoring word.\n",
    "            prev_tokens = T_output.argmax(-1)\n",
    "            all_out.append(prev_tokens)\n",
    "            all_attn.append(attn_probs)\n",
    "        \n",
    "        return torch.cat(all_out, dim=1), torch.stack(all_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention model\n",
    "\n",
    "The attention model computes a \"context\", which is a weighted sum over the encoder outputs. The weights $\\alpha_i$ are computed by an *energy* function that can make use of the decoder's state. This allows the attention model to select suitable information from the encoded sentence depending on what the decoder is doing.\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/waspnlp2020/ex3_1/attention.svg\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "To get started, we'll use a simpler attention mechanism than in Bahdanau's paper. In this case we won't use the decoder state: instead, we just hardcode all $\\alpha_i$ to $1/N$, where $N$ is the sentence length. That is, our representation of the encoded source sentence will be a *mean* of the encoder outputs for all positions. We'll implement the attention in a smarter way later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "                   \n",
    "    # When we implement the full attention mechanisms, we'll want to precompute the \"key\"\n",
    "    # representations. For now, we do nothing here.\n",
    "    def precompute_key(self, encoder_out):\n",
    "        return None\n",
    "            \n",
    "    def forward(self, query, precomputed_key, value, mask):\n",
    "        n_sen, n_words, enc_rnn_dim = value.shape\n",
    "        \n",
    "        # We initialize the \"energy\" scores to some constant value.\n",
    "        # (Doesn't matter which value, since we'll get 1/N after the softmax.)\n",
    "        scores = torch.zeros(n_sen, n_words, device=mask.device)        \n",
    "\n",
    "        # Mask out the energy scores for the padding tokens.\n",
    "        # We set them to -infinity so that they will be 0 after applying the softmax.\n",
    "        scores.data.masked_fill_(~mask, -float('inf'))\n",
    "                \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=1)        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas.unsqueeze(1), value)\n",
    "        \n",
    "        return context, alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the translation system\n",
    "\n",
    "We now have all the pieces to build the complete translation system. The implementation resembles the code in our previous exercises.\n",
    "\n",
    "The hyperparameters are stored in a container object `TranslatorParameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorParameters:\n",
    "    device = 'cuda'\n",
    "    \n",
    "    n_batches_print = 32\n",
    "    \n",
    "    src_voc_size = 10000\n",
    "    tgt_voc_size = 10000\n",
    "    \n",
    "    random_seed = 0\n",
    "            \n",
    "    n_epochs = 30\n",
    "    \n",
    "    batch_size = 128\n",
    "    \n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 0\n",
    "        \n",
    "    emb_dim = 256\n",
    "    \n",
    "    enc_rnn_dim = 512\n",
    "    enc_rnn_depth = 1\n",
    "\n",
    "    dec_rnn_dim = 512\n",
    "    dec_rnn_depth = 1\n",
    "    \n",
    "    max_train_sentences = 20000\n",
    "    max_valid_sentences = 1000\n",
    "    \n",
    "    src_train = 'de_en/de_en_train.de'\n",
    "    tgt_train = 'de_en/de_en_train.en'\n",
    "    src_valid = 'de_en/de_en_valid.de'\n",
    "    tgt_valid = 'de_en/de_en_valid.en'\n",
    "    test_sen = 'ich weiß nicht .'\n",
    "    \n",
    "    #src_train = 'sv_en/sv_en_train.sv'\n",
    "    #tgt_train = 'sv_en/sv_en_train.en'\n",
    "    #src_valid = 'sv_en/sv_en_valid.sv'\n",
    "    #tgt_valid = 'sv_en/sv_en_valid.en'\n",
    "    #test_sen = 'jag vet inte .'    \n",
    "      \n",
    "    \n",
    "class Translator:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        p = self.params\n",
    "        \n",
    "        # Setting a fixed seed for reproducibility.\n",
    "        torch.manual_seed(p.random_seed)\n",
    "        random.seed(p.random_seed)\n",
    "        \n",
    "        print('Preparing data...', end='')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Read the source-language data.\n",
    "        S_train = mt_util.read_lines(p.src_train, max_lines=p.max_train_sentences)\n",
    "        self.S_voc = mt_util.Vocabulary(include_unknown=True, lower=True, max_voc_size=p.src_voc_size)\n",
    "        self.S_voc.build(S_train)\n",
    "        S_valid = mt_util.read_lines(p.src_valid, max_lines=p.max_valid_sentences)\n",
    "\n",
    "        # Read the target-language data.\n",
    "        T_train = mt_util.read_lines(p.tgt_train, max_lines=p.max_train_sentences)\n",
    "        self.T_voc = mt_util.Vocabulary(include_unknown=True, lower=True, max_voc_size=p.tgt_voc_size)\n",
    "        self.T_voc.build(T_train)\n",
    "        T_valid = mt_util.read_lines(p.tgt_valid, max_lines=p.max_valid_sentences)\n",
    "                \n",
    "        # Batching for the training and validation sets.\n",
    "        self.batcher = mt_util.SequenceBatcher(p.device)        \n",
    "        train_dataset = mt_util.SequenceDataset(self.S_voc.encode(S_train), self.T_voc.encode(T_train))\n",
    "        train_loader = DataLoader(train_dataset, p.batch_size, shuffle=True, collate_fn=self.batcher)\n",
    "\n",
    "        valid_dataset = mt_util.SequenceDataset(self.S_voc.encode(S_valid), self.T_voc.encode(T_valid))\n",
    "        valid_loader = DataLoader(valid_dataset, p.batch_size, shuffle=True, collate_fn=self.batcher)\n",
    "        \n",
    "        print(' done.')\n",
    "        \n",
    "        print('Building model...', end='')\n",
    "        sys.stdout.flush()        \n",
    "        \n",
    "        # We use a utility to build the embedding layer, if we would like to \n",
    "        # use a pre-trained model (which we don't do here).\n",
    "        S_emb = self.S_voc.make_embedding_layer(finetune=True, emb_dim=p.emb_dim)\n",
    "        T_emb = self.T_voc.make_embedding_layer(finetune=True, emb_dim=p.emb_dim)\n",
    "        \n",
    "        # Build the encoder and decoder.\n",
    "        encoder = Encoder(S_emb, p)\n",
    "        decoder = Decoder(T_emb, p)\n",
    "        self.model = EncoderDecoder(encoder, decoder)\n",
    "        \n",
    "        self.model.to(p.device)\n",
    "        print(' done.')\n",
    "                \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), \n",
    "                                     lr=p.learning_rate, weight_decay=p.weight_decay)\n",
    "\n",
    "        # The loss function is a cross-entropy loss at the token level.\n",
    "        # We don't include padding tokens when computing the loss.\n",
    "        loss_func = torch.nn.CrossEntropyLoss(ignore_index=self.T_voc.get_pad_idx())\n",
    "\n",
    "        for epoch in range(1, p.n_epochs+1):\n",
    "            \n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "            for i, (Sbatch, Tbatch) in enumerate(train_loader, 1):\n",
    "                \n",
    "                # We use teacher forcing to train the decoder.\n",
    "                # This means that the input at each decoding step will be the\n",
    "                # *gold-standard* word at the previous position.\n",
    "                # We create a tensor Tbatch_shifted that contains the previous words.                \n",
    "                batch_size, sen_len = Tbatch.shape\n",
    "                zero_pad = torch.zeros(batch_size, 1, dtype=torch.long, device=Tbatch.device)\n",
    "                Tbatch_shifted = torch.cat([zero_pad, Tbatch[:, :-1]], dim=1)\n",
    "                \n",
    "                self.model.train()\n",
    "                scores = self.model(Sbatch, Tbatch_shifted)\n",
    "\n",
    "                loss = loss_func(scores.view(-1, len(self.T_voc)), Tbatch.view(-1))\n",
    "\n",
    "                optimizer.zero_grad()            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                # We periodically print some diagnostics: loss, BLEU score on the validation\n",
    "                # set, and the translation of a test sentence.\n",
    "                if i % p.n_batches_print == 0:\n",
    "                    test_out = ' '.join(self.translate([p.test_sen.split()])[0])\n",
    "                    bleu = self.eval_bleu(S_valid, T_valid)\n",
    "                    print(f' {i*p.batch_size}: loss={loss_sum/i:.4f} BLEU={bleu:.4f} | {p.test_sen} -> {test_out}')\n",
    "                    \n",
    "            print()\n",
    "            t1 = time.time()\n",
    "            \n",
    "            train_loss = loss_sum / len(train_loader)\n",
    "                                    \n",
    "            print(f'Epoch {epoch}: train loss = {train_loss:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "\n",
    "    # Utility function that runs the encoder and decoder for a batch.\n",
    "    # We then map the target-language word indices to the corresponding strings,\n",
    "    # and cut off the sentences after the end-of-sentence marker.\n",
    "    # For visualization, we'll also return all the attention scores.\n",
    "    def translate_batch(self, src):\n",
    "        with torch.no_grad():\n",
    "            encoded = self.S_voc.encode(src)\n",
    "            max_len = max(len(s) for s in encoded)\n",
    "            pad = self.S_voc.get_pad_idx()\n",
    "            for s in encoded:\n",
    "                s.extend([pad]*(max_len-len(s)))\n",
    "            enc_tensor = torch.tensor(encoded, device=self.params.device)\n",
    "            tgt, attn = self.model.greedy_decode(enc_tensor, max_len=2*max_len)\n",
    "            tgt_dec = self.T_voc.decode(tgt[:,1:].cpu().numpy())\n",
    "            for s in tgt_dec:\n",
    "                try:\n",
    "                    eos_ix = s.index(mt_util.EOS)\n",
    "                    del s[eos_ix:]\n",
    "                except:\n",
    "                    pass\n",
    "            return tgt_dec, attn.cpu().numpy()\n",
    "        \n",
    "    # Translates a set of sentences in the source language.\n",
    "    def translate(self, src, batch_size=512):\n",
    "        self.model.eval()\n",
    "        out = []\n",
    "        batch = []\n",
    "        for sen in src:\n",
    "            batch.append(sen)\n",
    "            if len(batch) == batch_size:\n",
    "                translated_batch, _ = self.translate_batch(batch)\n",
    "                out.extend(translated_batch)\n",
    "                batch = []\n",
    "        if len(batch) > 0:\n",
    "            translated_batch, _ = self.translate_batch(batch)\n",
    "            out.extend(translated_batch)\n",
    "        return out\n",
    "         \n",
    "    # Translates a single sentence and returns the translation and attention scores.\n",
    "    def translate_with_attention(self, sentence):\n",
    "        translated, attn = self.translate_batch([sentence])\n",
    "        attn = attn.squeeze(1)\n",
    "        return translated[0], attn\n",
    "        \n",
    "    # Runs the translator, and then uses the SacreBLEU evaluator to compute a BLEU score.\n",
    "    def eval_bleu(self, src, ref):\n",
    "        translated = self.translate(src)\n",
    "        translated = [' '.join(s) for s in translated]\n",
    "        ref = [' '.join(s).lower() for s in ref]\n",
    "        return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the system. While training, some diagnostics will be printed periodically: the training loss, the BLEU score on the validation set, and the translation of a sample sentence.\n",
    "\n",
    "Training the translator takes quite a bit of time. In the configuration parameters, you can reduce the number of epochs or training pairs. Or you might just terminate the training process after a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(TranslatorParameters())\n",
    "translator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating some sentences\n",
    "\n",
    "When you have trained a translator, you can try out some test sentences and see what happens. You can try some simple and some complex sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = ['ich bin hier .', 'das ist gut .', \n",
    "                  'das will ich nicht machen .', 'er hat eine kartoffel gegessen .']\n",
    "\n",
    "#test_sentences = [\n",
    "#    'Varför skulle vi inte dela med oss av dem ?',\n",
    "#    'Han kommer inte att kunna tala sig ur det gemensamma ansvaret så lätt .',\n",
    "#    'Det är i det sammanhanget som betänkandet skrivits .',\n",
    "#    'För det första , frigivning av alla politiska fångar .',\n",
    "#    'Idrotten har alltså lagts till .',\n",
    "#    'Det gör vi därför att vi är optimister .',\n",
    "#]\n",
    "\n",
    "translator.translate([s.split() for s in test_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3 in Bahdanau's paper shows some examples of attention scores in some selected sentences. Let's do something similar.\n",
    "\n",
    "We define a function `plot_attention` that visualizes the attention scores. The *x* axis corresponds to the source sentence and the *y* axis to the generated target sentence. The heatmap colors represent the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(src, trg, scores):\n",
    "\n",
    "    src = [mt_util.BOS] + src + [mt_util.EOS]\n",
    "    trg = [mt_util.BOS] + trg + [mt_util.EOS]\n",
    "    \n",
    "    scores = scores[:len(trg), :len(src)]\n",
    "\n",
    "    print(scores.shape)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(scores, cmap='Blues_r')\n",
    "\n",
    "    ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(trg, minor=False)\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try the attention visualization with the dummy attention we started the exercise with, you won't see any pattern since all the attention scores were set to be identical. This will be more interesting when you've implemented an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sen = 'das will ich nicht machen .'.split()\n",
    "\n",
    "trg_sen, attn = translator.translate_with_attention(src_sen)\n",
    "\n",
    "plot_attention(src_sen, trg_sen, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your programming tasks\n",
    "\n",
    "To get started, we used a simplified attention model that does not use the decoder state when selecting information from the encoded sentence. We'll now see if we can improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we don't use the encoder at all?\n",
    "\n",
    "As a warmup, before we implement a proper attention model, let's see how the decoder will behave when it receives no information at all from the encoder.\n",
    "\n",
    "Replace the class `MeanAttention` with something that returns a constant zero tensor and see what happens.\n",
    "\n",
    "What can you say about the sentences generated by the decoder now? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau's attention model\n",
    "\n",
    "There are some different ways to implement attention models. We'll describe how to implement the one described in [Bahdanau's paper](https://arxiv.org/pdf/1409.0473.pdf), but you can also consider alternative approaches (see below).\n",
    "\n",
    "Bahdanau's attention model is a feedforward model with one hidden layer, based on the decoder state and the encoder output. With a slight change in notation from the paper (Section A.1.2), the model looks like this:\n",
    "\n",
    "$$\n",
    "a(s_{i-1}, h_j) = v_a^{\\top} \\tanh(Q s_{i-1} + K h_j)\n",
    "$$\n",
    "\n",
    "Here, $h_j$ is an output from the encoded source sentence and $s_{i-1}$ the previous decoder RNN state. $Q$, $K$, and $v_a$ are matrices.\n",
    "\n",
    "In some discussions, attention is conceptualized in terms of a lookup operation with a *query*, *key*, and *value*. Intuitively, the query specifies what information to search for, the keys match the query, and the values are used to compute the result. In the model above, the representation $Q s_{i-1}$ corresponds to the *query*: it encodes what information we are searching for, based on the previous RNN state, while $K h_j$ corresponds to the *keys*. The values in this case are the encoder representations $h_j$, which we use to compute the aggregated representation.\n",
    "\n",
    "**Implementing the model**.\n",
    "\n",
    "Make a new class that has the same structure as `MeanAttention`. You will need to modify `precompute_key` and `forward`. \n",
    "\n",
    "`precompute_key` calculates the key representations ($K h_j$). In principle, this could be done inside `forward`, but since this part does not depend on the decoder state, we just have to compute it once. Use a linear model (`nn.Linear`) to compute this.\n",
    "\n",
    "`forward` has four inputs:\n",
    "* `query` of shape `(n_sentences, 1, decoder_rnn_size)`: this is the decoder state;\n",
    "* `precomputed_key` of shape `(n_sentences, n_src_words, hidden_size)`: this is what you computed previous.y;\n",
    "* `value` of shape `(n_sentences, n_src_words, encoder_rnn_size)`: this is the encoder output;\n",
    "* `mask` of shape `(n_sentences, n_src_words)`: to mask out padding tokens.\n",
    "\n",
    "Write code to compute the function `a` above. You will probably need linear models corresponding to $Q$ and $v_a$. Just like `scores` in `MeanAttention`, the result should have the shape `(n_sentences, n_src_words)`. The code that does the masking of padding tokens, softmax and weighted mean can be reused from `MeanAttention`.\n",
    "\n",
    "If you're confused about the tensor dimensions, like any human being, it might be helpful to print the shapes of the tensors while debugging.\n",
    "\n",
    "**Running the translator**.\n",
    "\n",
    "Retrain your translator and see how the BLEU scores are affected. When you have something that seems to work, visualize the attention scores on some sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other attention models\n",
    "\n",
    "[This document](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#summary) contains a nice summary of different ways to implement attention models. For example, the *scaled dot-product* attention, introduced by [Vaswani et al. (2017)](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), is well-known in the context of Transformers and BERT. Can you implement one or more of these attention variants?\n",
    "\n",
    "Or you might come up with something different? For instance, how about a deterministic attention mechansism where the attention weight $a_i$ is hardcoded to 1 if the decoder is at step $i$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
