{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and vectorize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample data (housing prices in Portland, OR) and store it into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "with open('portland.csv') as file:\n",
    "    for line in file:\n",
    "        cols = [int(c) for c in line.rstrip().split(',')]\n",
    "        xs.append(cols[:-1])\n",
    "        ys.append(cols[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the lists into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(xs)\n",
    "y = torch.FloatTensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the first few rows of the design matrix look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell prints the shape of the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how the corresponding values of the target vector look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line enables plotting into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(x[::,0], y)\n",
    "plt.xlabel('Size in square feet (x1)')\n",
    "plt.ylabel('Price in dollars (y)')\n",
    "plt.subplot(122)\n",
    "plt.scatter(x[::,1], y)\n",
    "plt.xlabel('Number of bedrooms (x2)')\n",
    "plt.ylabel('Price in dollars (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a linear model and train it using the mean squared error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Linear(2, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-7)\n",
    "\n",
    "for t in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = model.forward(x)\n",
    "    loss = F.mse_loss(output, y.view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the parameters (weights and biases) of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the prizes of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first few predicted prizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data again, this time with predictions from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(x[::,0], y)\n",
    "plt.xlabel('Size in square feet (x1)')\n",
    "plt.ylabel('Price in dollars (y)')\n",
    "plt.scatter(x[::,0], y_pred)\n",
    "plt.subplot(122)\n",
    "plt.scatter(x[::,1], y)\n",
    "plt.xlabel('Number of bedrooms (x2)')\n",
    "plt.ylabel('Price in dollars (y)')\n",
    "plt.scatter(x[::,1], y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next turn to a classical classification problem: handwritten digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and vectorize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST images and labels are stored as compressed bytestreams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def extract_images(file):\n",
    "    with gzip.open(file) as source:\n",
    "        source.read(16)  # skip header\n",
    "        return torch.FloatTensor(list(source.read())).view(-1, 784) / 255\n",
    "\n",
    "def extract_labels(file):\n",
    "    with gzip.open(file) as source:\n",
    "        source.read(8)  # skip header\n",
    "        return torch.LongTensor(list(source.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually have two different data sets: one for training, one for testing (validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = extract_images('train-images-idx3-ubyte.gz'), extract_labels('train-labels-idx1-ubyte.gz')\n",
    "print('Shapes of the training data matrices:', train_x.size(), train_y.size())\n",
    "\n",
    "test_x, test_y = extract_images('t10k-images-idx3-ubyte.gz'), extract_labels('t10k-labels-idx1-ubyte.gz')\n",
    "print('Shapes of the test data matrices:', test_x.size(), test_y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate our models using classification accuracy (percentage of correctly classified images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return torch.mean(torch.eq(y_pred, y).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the MNIST dataset is much bigger than the dataset with the housing prices, we will use stochastic gradient descent with minibatches. The following function splits the data into randomly sampled minibatches of the specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(x, y, batch_size):\n",
    "    random_indices = torch.randperm(x.size(0))\n",
    "    for i in range(0, x.shape[0] - batch_size + 1, batch_size):\n",
    "        batch_indices = random_indices[i:i+batch_size]\n",
    "        yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a softmax model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a softmax model and train it using the cross entropy loss function. Also, plot the per-epoch losses and the per-epoch accuracies on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal training loop\n",
    "\n",
    "def train_softmax(x, y, n_epochs=10, batch_size=50, eta=1e-1):\n",
    "    model = nn.Linear(784, 10)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    for t in range(n_epochs):\n",
    "        for bx, by in minibatches(x, y, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bx)\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same training loop with evaluation and plotting\n",
    "\n",
    "def train_softmax(x, y, n_epochs=10, batch_size=50, eta=1e-1, validation_data=None):\n",
    "    model = nn.Linear(784, 10)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for bx, by in minibatches(x, y, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bx)\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * len(bx)\n",
    "        losses.append(running_loss / len(x))\n",
    "        if validation_data:\n",
    "            test_x, test_y = validation_data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_y_pred = torch.argmax(model(test_x), axis=1)\n",
    "                acc = accuracy(test_y_pred, test_y)\n",
    "                accuracies.append(acc)\n",
    "                print('\\repoch {}, accuracy {:.4f}'.format(t, acc), end='')\n",
    "    print()\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average loss')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuracies)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test set accuracy')\n",
    "    plt.ylim([0.90, 1.00])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output layer of this network is not a softmax layer but just a linear layer. This means that the outputs for each class are not normalised probabilities but just scores. During training, these scores will be combined with a cross-entropy loss, which will implicitly compute the softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_softmax(train_x, train_y, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple, two-layer feed-forward neural network with a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal training loop\n",
    "\n",
    "def train_feedforward(x, y, n_epochs=10, batch_size=50, eta=1e-3):\n",
    "    model = FeedForwardNetwork(784, 392, 10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    for t in range(n_epochs):\n",
    "        for bx, by in minibatches(x, y, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bx)\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same training loop with evaluation and plotting\n",
    "\n",
    "def train_feedforward(x, y, n_epochs=10, batch_size=50, eta=1e-3, validation_data=None):\n",
    "    model = FeedForwardNetwork(784, 392, 10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for t in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for bx, by in minibatches(x, y, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bx)\n",
    "            loss = F.cross_entropy(output, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * len(bx)\n",
    "        losses.append(running_loss / len(x))\n",
    "        if validation_data:\n",
    "            test_x, test_y = validation_data\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_y_pred = torch.argmax(model(test_x), axis=1)\n",
    "                acc = accuracy(test_y_pred, test_y)\n",
    "                accuracies.append(acc)\n",
    "                print('\\repoch {}, accuracy {:.4f}'.format(t, acc), end='')\n",
    "    print()\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average loss')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(accuracies)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test set accuracy')\n",
    "    plt.ylim([0.90, 1.00])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feedforward(train_x, train_y, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
