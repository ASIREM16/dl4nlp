# Independent project

In the project work, your task will be to

* find an interesting language processing task that can be addressed with machine learning methods
* give a short talk that introduces your idea
* find (or annotate) an appropriate dataset for that task
* look around for related work
* implement software to train some machine learning model for your task
* evaluate your system
* write up a report about your results

The projects should normally be carried out individually. If you want to collaborate, please ask us for permission.

# Preliminary project description

Please submit a short text that describes what you intend to work on. For suggestions of topics, see below.

The text should include a project title and about 1 page giving a rough outline of how you intend to work. If you have found any related work or relevant dataset, this is useful information to include. The deadline for this preliminary description is **May 18**. We will send you our comments on **May 20**.

# Project pitch

In a Zoom session on May 25 or 26, each project idea will be presented in a 5-minute talk.

In your talk, the most important thing is to describe the task that you will work on. You may add a couple of examples to make the task a bit more understandable to someone who has never heard about it before. In addition, if you have found a dataset, you may want to mention a few details about it.

# The report

The text should be structured as a typical technical report, including an abstract, statement of problem, method description, experiments and results, and a conclusion. The deadline for submitting the report is **June 19**.

# Finding a topic

You can work on any project that is small and manageable enough for a short project of this kind. Ideally, try to find a topic that is relevant to your own research interests. It's OK if you end up with preliminary results, as long as you have some tidbit to present at the end.

If you need help deciding on a topic, here are a few suggestions. Some of these are very easy and some more challenging, and this list is not sorted by the difficulty level.

* **Adapting an assignment or exercise**. Start from one of the lab assignments or lecture demos and change it to something that is interesting for you. Maybe change assignment 1 to WSD for another language or another dataset? Maybe change the named entity exercise to another entity recognition task or a part-of-speech tagging task?
* **Comparing representations for an application**. Select some NLP application, either one we saw during the lectures or something where you can find the code online. Investigate how various types of text representations affect the application's performance. For instance, you could compare standard word embeddings, ELMo, BERT, T5, and other representations.
* **Caption generation**. Build a system that generates descriptions of images.
* **Try out a benchmark**. There are some popular benchmark sets such as [GLUE](https://gluebenchmark.com/) and the more recently released [Super GLUE](https://super.gluebenchmark.com/) that are used to evaluate language understanding systems. Build a model and evaluate it on one of these benchmarks.
* **Learning semantic categories**. Use a toolkit such as word2vec and a database of semantic categories (e.g. FrameNet) to build a system that discovers new words belonging to a category. For instance, you could make a system that discover words that refer to different types of food.
* **Crosslingual applications**. For some application, e.g. some categorization or tagging task we've seen in the course, investigate how well crosslingual representations such as multilingual BERT allows us to train with one language and evaluate with other languages.
* **Using reinforcement learning for parsing**. Train a transition-based dependency parser (or some other similar application) using reinforcement learning, for instance by following the approach described by [Fried and Klein (2018)](https://www.aclweb.org/anthology/P18-2075.pdf).
* **Shared tasks**. Every year, several NLP competitions are arranged at the [SemEval](http://alt.qcri.org/semeval2020/index.php?id=tasks) conference. Participate in one of the 2020 tasks or get the data from some task from previous years. Additionally, there are shared tasks organized by CoNLL, BioNLP and other conferences, where datasets can often be accessed easily. 

Alternatively, if you don't have an idea and you don't like the suggested topics, just talk to Marco and Richard. 

