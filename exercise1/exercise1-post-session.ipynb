{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Working with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version from the interactive session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to introduce you to some basic techniques for handling text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab is taken from [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), a collection of more than 100&nbsp;million tokens extracted from the set of &lsquo;Good&rsquo; and &lsquo;Featured&rsquo; articles on the English Wikipedia. More specifically, we will be using the training portion of the WikiText-2 dataset, which contains approximately 2&nbsp;million tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki.train.tokens', encoding='utf8') as fp:\n",
    "    wikitext = fp.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a line from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each line consists of a sequence of space-separated tokens. (In real applications, we would typically have to tokenise the raw text ourselves.) Here is a helper function that extracts these tokens from a given line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(lines):\n",
    "    for line in lines:\n",
    "        for token in line.rstrip().split():\n",
    "            yield token\n",
    "        yield '<eos>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode words into integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common task when processing text with neural networks is to encode words and other strings into contiguous ranges of integers so that we can map them to the components of a vector. The standard pattern for doing this looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "for token in tokens(wikitext):\n",
    "    if token not in stoi:\n",
    "        stoi[token] = len(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What index did the word *movie* get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we also need to invert the string-to-integer mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i: s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding should be the inverse of encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert itos[stoi['movie']] == 'movie'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see whether the word frequencies in the WikiText data follow the expected Zipfian pattern. We start by collecting the frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokens(wikitext))\n",
    "\n",
    "#counter = Counter(t.lower() for t in tokens(wikitext) if t.isalpha())    # for Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often does the word *movie* occur in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(counter['movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 10 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the word frequencies of the 100 most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels, values = zip(*counter.most_common(100))\n",
    "plt.loglog(range(len(labels)), values)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(*counter.most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are some problems for you to work on:**\n",
    "\n",
    "* Normalise your data by lowercasing and excluding non-alphabetical tokens. How does that affect the results?\n",
    "* Compute the size of the vocabulary and the total number of tokens. Compare your values to the official statistics on the WikiText website.\n",
    "* Check whether Heaps&rsquo; law seems to hold for the WikiText data by plotting the vocabulary size in steps of 1,000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary size:', len(stoi))\n",
    "\n",
    "print('Number of tokens:', sum(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = []\n",
    "tmp = set()\n",
    "for i, token in enumerate(tokens(wikitext)):\n",
    "    tmp.add(token)\n",
    "    if i % 1000 == 0:\n",
    "        values.append(len(tmp))\n",
    "\n",
    "plt.loglog(range(len(values)), values)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times when building neural network systems for text, we need to preprocess the data in some way (examples: tokenisation, stop word removal) and extract linguistic features from the text, such as part-of-speech tags or lemmas. For this we can use libraries such as [spaCy](https://spacy.io):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy comes with different languages models. Here we load the English language model. (There are actually several English language model; to save some time, we load the smallest one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a short text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u'Apple Corp. buys Alphabet Inc. for $1 billion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text using the default NLP pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the named entities (names of people, places, organisations, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dependency parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep', options={'distance': 110}, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are some problems for you to work on:**\n",
    "\n",
    "Read the [Linguistic Features](https://spacy.io/usage/linguistic-features) section from the spaCy documentation and solve the following problems on the WikiText data.\n",
    "\n",
    "* What is the percentage of stop words in the data? (Stop words are frequent words that we may wish to ignore.)\n",
    "* Compile a small dictionary with all verbs in the data. Represent verbs by their lemmas.\n",
    "* Extract the names of all people mentioned in the data. Do you notice any obvious errors made by the named entity recogniser?\n",
    "* Extract all two-word compounds in the data. How many compounds do you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample solutions to the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the WikiText data and store the processed \"documents\" in a list\n",
    "\n",
    "docs = list(nlp.pipe(wikitext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "k = 0\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        n += 1\n",
    "        k += token.is_stop\n",
    "print('Percentage of stop words: {:.2f}'.format(k / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = set()\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if token.text.islower() and token.is_alpha and token.pos_ == 'VERB':\n",
    "            verbs.add(token.lemma_)\n",
    "print('Size of the vocabulary:', len(sorted(verbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = set()\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            ents.add(ent.text)\n",
    "print(sorted(ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = set()\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'compound':\n",
    "            if token.head.i < token.i:\n",
    "                compounds.add(token.head.text + ' ' + token.text)\n",
    "            else:\n",
    "                compounds.add(token.text + ' ' + token.head.text)\n",
    "print(sorted(compounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That&rsquo;s all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
