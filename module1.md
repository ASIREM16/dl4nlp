# Module 1

In the first module, we introduce the most important building blocks used in modern NLP architectures. We will also consider one of the most fundamental NLP tasks: text categorization.

## Review

We assume that you have background knowledge in machine learning and neural networks and practical exposure to PyTorch. If you need to brush up this up a bit, then you can look at the following background/review material:

* Machine learning basics, deep feedforward networks [[playlist](https://www.youtube.com/playlist?list=PLvWwkcdbWwLWq2H9Zs1Ze91oE0kJN8OD_)]
* PyTorch 60 Minute Blitz [[tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)]
* Neural models with PyTorch [[videos](https://www.youtube.com/watch?v=XqUCkf7Muuw&list=PLvWwkcdbWwLVi7Nb9RK410ApKdHsHhxtO&index=3&t=0s)] [[notebook and data](https://github.com/liu-nlp/dl4nlp/tree/master/background)]

**Reading:** Goldberg, chapter 1–7 (review)

## Lecture 1

**Form:** Scheduled, 30 March, 13:15–14:00, [Zoom](https://liu-se.zoom.us/j/417912565)

* Welcome! Practical matters, assessment
* Overview of the content of the course
* Capsule introduction to NLP [[slides](slides/slides-111.pdf)] [[video](https://youtu.be/6u7u1cpVT7Y)]

**Reading:** Eisenstein, chapter 1

## Lecture 2

**Form:** Self-study

* Essentials of linguistics [[slides](slides/slides-121.pdf)] [[video](https://youtu.be/riYFhZj_CMg)]
* Introduction to text classification [[slides](slides/slides-122.pdf)] [[video](https://youtu.be/3yeOoKhiy8A)]
* Evaluation of text classifiers [[slides](slides/slides-123.pdf)] [[video](https://youtu.be/YPq1Ztr-AAI)]

**Reading:** Eisenstein, sections 2.1, 2.3, 2.5, 2.6; chapter 4

## Exercise 1

**Form:** Scheduled, 31 March, 10:15–12:00, Zoom

* Working with text practically

## Lecture 3

**Form:** Self-study

* Representing documents for neural networks [[slides](http://www.cse.chalmers.se/~richajo/waspnlp2020/m1_3/m3_1.pdf)] [[video](https://youtu.be/xsQ46CXsIfc)]
* Introduction to embeddings [[slides](http://www.cse.chalmers.se/~richajo/waspnlp2020/m1_3/m3_2.pdf)] [[video](https://youtu.be/LLUjsmuEgk8)]
* Continuous bag-of-words representations [[slides](http://www.cse.chalmers.se/~richajo/waspnlp2020/m1_3/m3_3.pdf)] [[video](https://youtu.be/MOcGoA3Fbi8)]
* Examples [[notebook](http://www.cse.chalmers.se/~richajo/waspnlp2020/m1_3/Document%20classification.ipynb)] [[colab](https://drive.google.com/file/d/1VLIAYXSoLN99BwS9CUTJYS7caazBVORF/view?usp=sharing)] [[video](https://youtu.be/ZEYESgSR29o)]

**Reading:** Goldberg, chapter 8; [Joulin et al. (2017)](https://aclweb.org/anthology/E17-2068)

## Exercise 2

## Lecture 4

**Form:** Self-study

* Pre-training word embedding models
* Applying pre-trained embedding models

## Exercise 3

## Lecture 5

**Form:** Self-study

* Convolutional neural networks
* Recurrent neural networks

## Lecture 6

**Form:** Self-study

* More recurrent architectures
* Attention models
* Language models

## Lecture 7

**Form:** Self-study

* Transfer learning in NLP
* Transformer models
* Pre-training deep models (ELMo, BERT)

## Exercise 4

## Programming assignment

[The first programming assignment](http://www.cse.chalmers.se/~richajo/waspnlp2020/a1/assignment1.html) is dedicated to the task of *word sense disambiguation*.
     
