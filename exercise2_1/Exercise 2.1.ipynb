{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1: Named entity recognition\n",
    "\n",
    "In this exercise, we will implement a named entity recognition system using a few different neural models. This example uses training and validation data from the [CoNLL-2003 Shared Task](https://www.clips.uantwerpen.be/conll2003/ner/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this notebook less cluttered, we move some utilities for preprocessing, batch management, and evaluation into a separate Python file `ner_util.py`.\n",
    "\n",
    "The main utilities in this file are:\n",
    "* `read_data` for reading the dataset,\n",
    "* `Vocabulary` for managing the vocabulary,\n",
    "* `SequenceDataset` and `SequenceBatcher` for managing minibatches,\n",
    "* `load_gensim_vectors` for loading pre-trained word embeddings via [gensim](https://radimrehurek.com/gensim/),\n",
    "* `evaluate_bio` for computing evaluation scores for the predicted entities,\n",
    "* `show_entities` for printing sentences and their entities in a nice format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ner_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The training, validation and testing data can be downloaded from the following site:\n",
    "\n",
    "http://demo.spraakdata.gu.se/richard/dl4nlp/ex2_1/\n",
    "\n",
    "The username and password are both `waspnlp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note about the format. The dataset consists of tokenized sentences. Each row corresponds to one token, and for each token there is a word a number of annotations, separated by whitespace. The sentences are separated by empty lines. Here is an example of a sentence.\n",
    "```\n",
    "United   NNP B-NP B-ORG\n",
    "Nations  NNP I-NP I-ORG\n",
    "official NN  I-NP O\n",
    "Ekeus    NNP B-NP B-PER\n",
    "heads    VBZ B-VP O\n",
    "for      IN  B-PP O\n",
    "Baghdad  NNP B-NP B-LOC\n",
    ".        .   O    O\n",
    "```\n",
    "In this exercise, we will just use the first and last columns: the words and the BIO-coded named entity labels. (The second and third columns contain part-of-speech tags and phrase labels, but we will ignore them.)\n",
    "\n",
    "The utility function `read_data` reads a file and returns the words and BIO labels for all sentences in a file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Getting started with a baseline model for sequence labeling\n",
    "\n",
    "We first define the neural network model.\n",
    "\n",
    "Our first model is simplistic and will be extended later. To predict the output BIO tag for a word, this model applies an embedding and an output unit and it does not consider the context of the word. As we have seen in one of the lectures, this is likely to work quite poorly since words may behave differently in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_emb_layer, n_labels):\n",
    "        super().__init__()                \n",
    "\n",
    "        # The model consists of just a word embedding layer and a\n",
    "        # linear output unit. The embedding layer has already been created elsewhere.        \n",
    "        self.word_embedding = word_emb_layer\n",
    "        \n",
    "        word_emb_dim = word_emb_layer.weight.shape[1]\n",
    "        \n",
    "        self.top_layer = nn.Linear(word_emb_dim, n_labels)\n",
    "                        \n",
    "    def forward(self, words):\n",
    "        # words is a tensor of integer-encoded words, with shape (n_sentences, n_words)\n",
    "                \n",
    "        # After embedding the words, the shape is (n_sentences, n_words, emb_dim). \n",
    "        word_repr = self.word_embedding(words)\n",
    "            \n",
    "        # We predict the BIO label simply by applying a linear model to\n",
    "        # the word embedding at that position.\n",
    "        \n",
    "        # The shape of the output is (n_sentences, n_words, n_labels),\n",
    "        # where n_labels is the size of the output label vocabulary.\n",
    "        return self.top_layer(word_repr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the complete the system\n",
    "\n",
    "We can now assemble all the pieces and build a complete named entity recognition system, implemented in the class `SequenceLabeler`. \n",
    "\n",
    "The interesting part here is the method `train`, which carries out the preprocessing steps, sets up the neural network defined above, and then runs the training loop. You should be familiar with the general structure of this kind of programs by now. The comments inside the code explain the details more explicitly.\n",
    "\n",
    "The method `predict` can be used to apply the trained NER system to new sentences.\n",
    "\n",
    "The hyperparameters are stored in a container `SeqLabelerParameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqLabelerParameters:\n",
    "    device = 'cuda'\n",
    "    \n",
    "    random_seed = 0\n",
    "    \n",
    "    train_file = YOUR_TRAINING_FILE\n",
    "    valid_file = YOUR_VALIDATION_FILE\n",
    "    \n",
    "    use_characters = False\n",
    "    \n",
    "    word_emb_dim = 128\n",
    "    \n",
    "    pretrained_word_emb = None\n",
    "    finetune_word_emb = True\n",
    "    \n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    \n",
    "    learning_rate = 5e-3\n",
    "    weight_decay = 1e-6\n",
    "    \n",
    "    word_dropout_prob = 0\n",
    "    \n",
    "\n",
    "class SequenceLabeler:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params        \n",
    "                \n",
    "    def train(self):\n",
    "        \n",
    "        p = self.params\n",
    "        \n",
    "        # Setting a fixed seed for reproducibility.\n",
    "        torch.manual_seed(p.random_seed)\n",
    "        random.seed(p.random_seed)\n",
    "        \n",
    "        # Read training and validation data according to the predefined split.\n",
    "        Xtrain, Ytrain = ner_util.read_data(p.train_file)\n",
    "        Xval, Yval = ner_util.read_data(p.valid_file)\n",
    "                \n",
    "        # Create vocabularies for words and output labels.\n",
    "        self.word_voc = ner_util.Vocabulary(include_unknown=True, gensim_model=p.pretrained_word_emb)\n",
    "        self.label_voc = ner_util.Vocabulary(include_unknown=False)\n",
    "        self.label_voc.build(Ytrain)\n",
    "        \n",
    "        # If we are using a pre-trained word embedding model, then we use its built-in vocabulary;\n",
    "        # otherwise, we build the word vocabulary from the data.\n",
    "        if not p.pretrained_word_emb:\n",
    "            self.word_voc.build(Xtrain)        \n",
    "\n",
    "        # Also create a vocabulary for characters. (Will be used in a later part of the exercise.)\n",
    "        self.char_voc = ner_util.Vocabulary(include_unknown=True, character=True)            \n",
    "        self.char_voc.build(Xtrain)\n",
    "        \n",
    "        # Put the training and validation data into Datasets and DataLoaders for managing minibatches.\n",
    "        self.batcher = ner_util.SequenceBatcher(p.device)        \n",
    "        train_dataset = ner_util.SequenceDataset(self.word_voc.encode(Xtrain),                                        \n",
    "                                        self.label_voc.encode(Ytrain),\n",
    "                                        self.char_voc.encode(Xtrain) if p.use_characters else None,\n",
    "                                        word_dropout_prob=p.word_dropout_prob, \n",
    "                                        word_dropout_id=self.word_voc.get_unknown_idx())\n",
    "        train_loader = DataLoader(train_dataset, p.batch_size, shuffle=True, collate_fn=self.batcher)        \n",
    "        val_dataset = ner_util.SequenceDataset(self.word_voc.encode(Xval), \n",
    "                                      self.label_voc.encode(Yval),\n",
    "                                      self.char_voc.encode(Xval) if p.use_characters else None)\n",
    "        val_loader = DataLoader(val_dataset, p.batch_size, shuffle=False, collate_fn=self.batcher)\n",
    "        \n",
    "        \n",
    "        # Now, let's build the model!\n",
    "\n",
    "        # First, we create a word embedding layer. We use another utility function for that (in\n",
    "        # the Vocabulary object). The reason we are doing it in this way is just to simplify our\n",
    "        # work if we want to use pre-trained embeddings later on.\n",
    "        # For now, this will just build a straightforward nn.Embedding and return it.\n",
    "        emb_layer = self.word_voc.make_embedding_layer(finetune=p.finetune_word_emb, \n",
    "                                                       emb_dim=p.word_emb_dim)\n",
    "    \n",
    "        # Create the sequence labeling neural network defined above.\n",
    "        self.model = SimpleSequenceModel(emb_layer, n_labels=len(self.label_voc))\n",
    "        \n",
    "        self.model.to(p.device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), \n",
    "                                     lr=p.learning_rate, weight_decay=p.weight_decay)\n",
    "\n",
    "        # Cross-entropy loss function that we will use to optimize the model.\n",
    "        # In particular, note that by using ignore_index, we will not compute the loss \n",
    "        # for the positions where we have a padding token.\n",
    "        loss_func = torch.nn.CrossEntropyLoss(ignore_index=self.label_voc.get_pad_idx())\n",
    "        \n",
    "        history = defaultdict(list)\n",
    "                            \n",
    "        for i in range(p.n_epochs):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "            for Xbatch_words, Ybatch, Xbatch_chars in train_loader:\n",
    "                                                \n",
    "                # Compute the output scores.\n",
    "                scores = self.model(Xbatch_words)\n",
    "                \n",
    "                # The scores tensor has the shape (n_sentences, n_words, n_labels).\n",
    "                # We reshape this to (n_sentences*n_words, n_labels) because the loss requires\n",
    "                # a 2-dimensional tensor. Similar for the gold standard label tensor.                \n",
    "                loss = loss_func(scores.view(-1, len(self.label_voc)), Ybatch.view(-1))\n",
    "                    \n",
    "                optimizer.zero_grad()            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "            train_loss = loss_sum / len(train_loader)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            \n",
    "            # Evaluate on the validation set.\n",
    "            stats = defaultdict(Counter)\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for Xbatch_words, Ybatch, Xbatch_chars in val_loader:\n",
    "                    scores = self.model(Xbatch_words)\n",
    "                    \n",
    "                    # Compute the highest-scoring labels at each word position.\n",
    "                    predicted = scores.argmax(dim=2)\n",
    "                    \n",
    "                    # Update the evaluation statistics for this batch.\n",
    "                    ner_util.evaluate_iob(Xbatch_words, predicted, Ybatch, self.label_voc, stats)\n",
    "\n",
    "            # Compute the overall F-score for the validation set.\n",
    "            _, _, val_f1 = ner_util.prf(stats['total'])\n",
    "\n",
    "            history['val_f1'].append(val_f1)\n",
    "\n",
    "            t1 = time.time()\n",
    "            print(f'Epoch {i+1}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "           \n",
    "        # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "        # precision, recall, and F-scores for the different types of named entities.\n",
    "        print()\n",
    "        print('Final evaluation on the validation set:')\n",
    "        p, r, f1 = ner_util.prf(stats['total'])\n",
    "        print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        for label in stats:\n",
    "            if label != 'total':\n",
    "                p, r, f1 = ner_util.prf(stats[label])\n",
    "                print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        \n",
    "        self.stats = stats\n",
    "        \n",
    "        plt.plot(history['train_loss'])\n",
    "        plt.plot(history['val_f1'])\n",
    "        plt.legend(['training loss', 'validation F-score'])\n",
    "\n",
    "        \n",
    "    def predict(self, sentences):\n",
    "        # This method applies the trained model to a list of sentences.\n",
    "        \n",
    "        Ydummy = [[self.label_voc.itos[0]]*len(sen) for sen in sentences]\n",
    "            \n",
    "        dataset = ner_util.SequenceDataset(self.word_voc.encode(sentences), \n",
    "                                  self.label_voc.encode(Ydummy),\n",
    "                                  self.char_voc.encode(sentences))\n",
    "        loader = DataLoader(dataset, self.params.batch_size, shuffle=False, collate_fn=self.batcher)\n",
    "                \n",
    "        out = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for Xbatch_words, _, Xbatch_chars in loader:\n",
    "                \n",
    "                scores = self.model(Xbatch_words)                   \n",
    "                predicted = scores.argmax(dim=2) \n",
    "                \n",
    "                # Convert the integer-encoded tags to tag strings.\n",
    "                for pred_sen in predicted.cpu().numpy():\n",
    "                    tokens = sentences[len(out)]\n",
    "                    out.append([self.label_voc.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the complete system. The training loop will compute the loss during each epoch, and then compute the F-score (the harmonic mean of precision and recall) on the validation set.\n",
    "\n",
    "After completing the training process, we get some more detailed evaluation results, where you can see precision, recall, and F-scores for all types of entities. After 10 epochs with the default system, we get an F-score of about 0.60, so the system has learned to pick up some named entities although it is far from perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_system = SequenceLabeler(SeqLabelerParameters())\n",
    "ner_system.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out some examples\n",
    "\n",
    "Let's consider some examples and try to understand how the system behaves.\n",
    "\n",
    "As mentioned above, we can call `predict` to get the system's predictions for a new text. The text needs to be split into sentences and tokens. The corresponding BIO labels will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_system.predict(['Jane lives in New York City .'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a utility function `show_entities` that shows the sentence and the entities in a colored format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_util.show_entities(ner_system, ['Jane lives in New York City .'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets more interesting when we include some words that should be tagged differently depending on the context. Here, we can clearly see the limitations of our current model. (In the second example, we'd like *Manchester* to be an organization in the first case and a location in the second case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_util.show_entities(ner_system, ['Manchester United will return to the United States .'.split(),\n",
    "                                    'Manchester scored against Liverpool when they were playing in Manchester .'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also note that the system generalizes poorly when we encounter words that have not been observed in the training set. Our system does not find any names in the example below, but as human readers, we don't have a problem to understand that *Tarakanov* is a person and *Lüneburg* is a location.\n",
    "\n",
    "For instance, we might already know from prior experience that *Tarakanov* is a surname, or we can deduce this because of the *-ov* suffix or because the context makes it likely that this is a person. Similarly, we might have heard about *Lüneburg* (a city in Germany) before, or we guess that this is a place because of the context or the *-burg* suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_util.show_entities(ner_system, ['Tarakanov lives in Lüneburg .'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Window-based sequence labeling\n",
    "\n",
    "Our first model works poorly because it does not take the surrounding words into account when predicting the output labels. So let's simply build a second prediction model that not only looks at a single word in isolation, but also considers one word before and one word after.\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/waspnlp2020/ex2_1/ffwin_simpler.svg\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "**Hint**: To save you some time in the exercise, here is some code that might be useful when you compute the representations of the three-word windows. We assume that we have computed a word embedding tensor `word_repr` for our batch as in the code above, which will then have the shape `(n_sentences, n_words, emb_dim)`.\n",
    "\n",
    "We can then create a tensor `before_word_repr` that contains word embeddings shifted one step backward, and another tensor `after_word_repr` where the embeddings have been shifted one step forward. We insert some zero padding so that these tensors will have the same shape as `word_repr`.\n",
    "\n",
    "    n_sent, _, emb_dim = word_repr.shape\n",
    "    zero_pad = torch.zeros(n_sent, 1, emb_dim, device=word_repr.device)\n",
    "    word_before_repr = torch.cat([zero_pad, word_repr[:,:-1,:]], dim=1)\n",
    "    word_after_repr = torch.cat([word_repr[:,1:,:], zero_pad], dim=1)\n",
    "\n",
    "Then \"glue\" the three word embedding tensors together using `torch.cat` so that you get a tensor representing the 3-word windows. This will then have the shape `(n_sentences, n_words, 3*emb_dim)`.\n",
    "\n",
    "Finally, modify the output unit to reflect this change and rerun the code. Do the scores improve? How about the test cases above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. RNN-based sequence labeling\n",
    "\n",
    "The window-based prediction model is limited in its representation capabilities by the width of the window, so let's try to use a RNN-based representation, which does not have this limitation. The following figure gives you an idea of the type of model we should try to build: \n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/waspnlp2020/ex2_1/rnn_seq_ner.svg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Hint 1**: The code to build the model and the `forward` step will probably be quite similar to what you wrote in the language modeling exercise.\n",
    "\n",
    "**Hint 2**: a bidirectional RNN will probably be more useful than the model in the figure. Any idea why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Pre-trained word embeddings\n",
    "\n",
    "How can we deal with names that we haven't seen before? For instance, in the following examples, the names are not present in the training set, and the context doesn't give us much information:\n",
    "\n",
    "    Linköping is nice.\n",
    "    Judy is nice.\n",
    "\n",
    "Currently, since *Linköping* and *Judy* haven't been seen before, their representations will be identical, since we will use a generic \"unknown word\" embedding in these cases. Pre-trained word embeddings may help us here: while *Linköping* is not present in our training set, it is distributionally similar to names of other cities, and this might help us to assign the correct label `B-LOC`.\n",
    "\n",
    "The [gensim](https://radimrehurek.com/gensim/) library includes a number of built-in word embedding models. You can find a list of them here:\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim-data#models\n",
    "\n",
    "The utility function `load_gensim_vectors` loads a pre-trained word embedding model and converts it into a PyTorch tensor. Gensim will automatically download the file (which will take some minutes the first time). For instance:\n",
    "\n",
    "    gensim_glove_model = ner_util.load_gensim_vectors('glove-wiki-gigaword-100', builtin=True)\n",
    "\n",
    "Here, `glove-wiki-gigaword-100` is the name of the model, and `builtin=True` shows that we are using one of the library's built-in models (and not a file of our own).\n",
    "\n",
    "Load one of gensim's pre-trained models. Then, in the hyperparameter settings defined above, change `pretrained_word_emb = None` to the embedding model you loaded. Retrain the model and see if this improves the results. Can the system handle the problematic examples above now?\n",
    "\n",
    "**Hint**: it might be useful to investigate whether the word embedding model should be *fine-tuned* or not. This can be controlled via the hyperparameter `finetune_word_emb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Character-based word representation\n",
    "\n",
    "Although we have improved the coverage by using the pre-trained word embeddings, the model still lacks a way to generalize in a way that takes word properties into account. For instance, as mentioned in the lecture, many Swedish surnames end in *-sson*, which could help us deduce that such strings are person names. Or on a more basic level, we may observe that an initial upper-case letter makes it more likely that a word is a name. By introducing a *character-based* word representation, our model might learn to generalize in this way.\n",
    "\n",
    "The following figure gives you an idea of what we would like to implement:\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/waspnlp2020/ex2_1/char_repr.svg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "The \"something\" in this figure could be implemented in different ways:\n",
    "\n",
    "* In [*Neural Architectures for Named Entity Recognition*](https://www.aclweb.org/anthology/N16-1030.pdf) by Lample et al. (2016), a bidirectional RNN is applied to the characters. (See Figure 4.)\n",
    "* In [*End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF*](https://www.aclweb.org/anthology/P16-1101.pdf) by Ma and Hovy (2016), a convolutional layer and pooling is used. (See Figure 1.)\n",
    "\n",
    "To work with characters in our code, first set the hyperparameter `use_characters` to `True`. By enabling this option, we will include character tensors in each minibatch. (That is, `Xbatch_chars` in the training loop will be non-empty.)\n",
    "\n",
    "Then create a new model where the `forward` method takes the word and character tensors as the input, and implement a word representation approach that combines word embeddings with a character-based representation. Do you get better F-scores now? Also try to think of some test cases where it might be useful to take the word structure into account and investigate if your model can handle them better now than before.\n",
    "\n",
    "**Hints**:\n",
    "* RNN and CNN layers expect three-dimensional inputs. The character tensor in each batch has the shape `(n_sentences, n_words, n_characters)`, so you will likely have to reshape it into `(n_sentences*n_words, n_characters)` to get a 3-dimensional structure after embedding the characters.\n",
    "* If you'd like to implement Lample's RNN-based architecture, you might have something like the following:\n",
    "```\n",
    "    _, (final_state, _) = char_rnn(character_embedding)  # if char_rnn is an LSTM\n",
    "    _, final_state      = char_rnn(character_embedding)  # if char_rnn is a GRU\n",
    "``` \n",
    "  In this case, `final_state` will be a tensor with the shape `(n_layers, batch_size, rnn_size)`. It contains the final RNN states in each layer for every sequence in the batch. To follow Lample's approach, you can extract the last two \"rows\" in this tensor.\n",
    "* For a CNN-based solution as in Ma's architecture, you should use a one-dimensional convolution (`nn.Conv1d`) followed by a pooling operation. Note that the convolution is computed over the last dimension of the tensor, so you might need to transpose the tensor to put the character dimension last: `character_embedding.transpose(1, 2)`. To carry out a pooling operation over the entire sequence, you can use *adaptive* pooling layers, e.g. `nn.AdaptiveMaxPool1d(1)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
